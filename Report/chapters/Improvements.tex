\section{Our Improvements}\label{sec:improvements}

In this chapter, we introduce some modifications or additions to the solution proposed by Peng et al. in \cite{peng_main_paper}, aiming to improve the robot's behavior in more general and complex environments.

\subsection{Subgoals and RRT*}
As described in Section \ref{sec:mpc}, the solution provided by the MPC is the input vector that minimizes the cost function while satisfying all the constraints. When an obstacle lies between the start and the goal positions (as illustrated in Figure \ref{fig:sim_rrt_env-red-star}), the humanoid must get around it in order to approach the goal. However, it would require the robot to deliberately increase its distance from the goal before reducing it. It implies that the MPC should return a sub-optimal solution, though one that minimizes the cost function exists. Consequently, an approach that relies solely on the MPC cannot guarantee goal attainment in such cases.\\
To address this, we compute a path from the start to the goal position using RRT*. Then, we request the humanoid to reach sequentially all the \textit{subgoals}, namely the nodes of the tree in the path from the start to the goal. In this way, we ensure that the MPC produces feasible control inputs while adhering to the original cost-minimization framework, and the humanoid can successfully reach the goal.\\
The RRT* algorithm is used to rapidly compute a collision-free path while minimizing its cost, i.e. the sum of the weights on the edges connecting the start to the goal node. Whenever a new node $j$ is added to the tree, the cost of the edge $e_{i,j}$ connecting it to node $i$ is defined as:
$$ cost(e_{i,\, j}) \coloneqq  cost(path_{i,\,start})\, +\,dist(i,\, j) * e^{-clearance(j)}. $$
It is the sum of two addends: the cost of the path from $i$ to the start node, and the Euclidean distance between the position represented by $i$ and the one represented by $j$, multiplied by the exponential of the negative clearance of the new node (namely, the distance between $j$ and the closest obstacle). Hence, the resulting path will minimize the travelled distance while maximizing the clearance.

Figures \ref{fig:sim_rrt_env-red-star} and \ref{fig:sim_rrt_no_rrt_end} show the behavior of the humanoid in a tricky environment employing the framework without the RRT* extension. To reach the goal, the robot must first overcome the obstacle by passing through the via-point represented by the red star. However, for the reasons explained above, this is not possible. Thus, the MPC only tries to reduce the distance from the goal, and leads the humanoid toward the obstacle, where it gets stuck because it cannot get closer to the goal without colliding.\\
Following the approach that integrates RRT*, the robot is able to reach the goal (Figure \ref{fig:sim_rrt_frames}). The MPC is requested to provide the inputs that drive the humanoid through the sequence of subgoals. In this way, the robot can pass by the red star, overcome the obstacle, and finally reach the goal.

From the evolution of the state (illustrated in Figure \ref{fig:sim_rrt_evol}) emerges something that never occurred in the previous simulations. When the robot relies only on the MPC solutions to approach the goal, the error curves monotonically decrease (except for minimal variations). On the other hand, as described at the beginning of this section, in this case, it is necessary to overcome the obstacle, and hence increase the error. When the approach integrating RRT* is used, the humanoid moves toward the first subgoal, and initially increases the magnitude of the error with the final goal. By traveling through the RRT*-defined subgoals, the error is progressively reduced until the goal is reached.\\
The \textit{step line} in the orientation plot indicates that, whenever a subgoal is reached, a turning rate is commanded to point toward the next subgoal. This behavior explains the peaks in the $\omega$ plot. The high lateral velocity generated while turning arises for the same reason outlined in Section \ref{subsec:sim_simple_env}.


\begin{figure}[H]
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{figures//Simulations//sim_rrt/env_redstar.pdf}
        \caption{This figure illustrates an environment where the humanoid cannot reach the goal unless it employs the framework integrating RRT*. The robot starts from position $(0,0)$ with orientation $\theta=0$, and the goal is at $(5,5)$. The red star represents an ideal via-point to arrive at the goal.}
    \label{fig:sim_rrt_env-red-star}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{figures//Simulations//sim_rrt/no_rrt_end.pdf}
        \caption{This figure shows what happens when the robot tries to reach the goal in a tricky environment with the base framework.}
    \label{fig:sim_rrt_no_rrt_end}
\end{subfigure}
\caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures//Simulations//sim_rrt/rrt_res.pdf}
    \caption{This figure illustrates the tree computed by the RRT* algorithm. The path is represented as a red line, the start position as a red point, and the goal as a blue star. The RRT is executed and illustrated inside an occupancy grid representing the original (continuous) workspace.}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    % First row
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Simulations/sim_rrt/frame_0.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Simulations/sim_rrt/frame_2.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Simulations/sim_rrt/frame_4.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Simulations/sim_rrt/frame_6.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Simulations/sim_rrt/frame_9.pdf}
    \end{subfigure}
    
    \caption[short]{This sequence of frames illustrates the robot's trajectory to the goal passing through the RRT*-defined subgoals (represented as orange points).}
    \label{fig:sim_rrt_frames}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Simulations/sim_rrt/evolution_0.pdf}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Simulations/sim_rrt/evolution_1.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Simulations/sim_rrt/evolution_2.pdf}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Simulations/sim_rrt/evolution_3.pdf}
    \end{subfigure}
    \caption{These figures depict the evolution of the humanoid's state and the input throughout the simulation with RRT*. The top-left plot shows the error between the CoM and the goal position, while the top-right represents the translational velocities along the x- and y-axis. The bottom left and right plots show the theta and omega evolution, respectively. All the quantities are expressed in the global RF.}
    \label{fig:sim_rrt_evol}
\end{figure}




\subsection{Unknown Environment}
In unknown environments, the robot does not rely on a pre‐loaded map but rather builds its understanding on the fly.
\todo[inline]{proposta cambiare con "the robot navigation is not based on a pre-loaded map, but on real-time perceptions."}
To achieve this, a 2D LiDAR range finder is employed to capture real-time measurements of the surroundings.
\todo{proposta cambiare con: "These LiDAR readings are affected by a zero-mean Gaussian noise"???} These LiDAR readings are getting perturbed with zero-mean Gaussian noise to better simulate the uncertainty found in real-world sensor data.
The noisy measurements are then processed using the DBSCAN clustering algorithm (with \todo{$\epsilon = 0.3$?} an epsilon of 0.3) to group nearby points into clusters that represent inferred obstacles.

This leads to a further tuning of the system, in order to achieve desired performance in real world scenarios; \todo[inline]{proposta cambiare con: "With the integration of a LiDAR system, our framework is capable of replicating the previously described performance in real-word scenarios as well."}
in fact provided that every real obstacle is convex, and assuming the
obstacles are sufficiently distant, the LiDAR resolution is not too high (here 360°), and the DBSCAN epsilon
remains small enough (here 0.3); these inferred obstacles will also be convex.
\todo[inline]{In fact, provided that every obstacle in the workspace is convex, and assuming that the obstacles
are sufficiently distant, the LiDAR resolution is about 360°, and the
DBSCAN $\epsilon$ is small enough, the obstacles inferred by our system will
be convex too.}
By considering only these clustered obstacles—rather than every single detected point—the navigation system mirrors the behavior of the base simulation while accounting for realistic environmental complexity.
\todo[inline]{proposta cambiare con: (continuando nello stesso paragrafo) "Then, the MPC computes one LDCBF for each of those obstacles. In this way, the navigation system can mirror ..."}
\todo{", and the humanoid" continuando da precedente paragrafo} In this way, the humanoid robot is able to navigate safely despite not knowing the environment a priori, relying solely on the \todo{obstacles inferred dynamically} dynamically inferred obstacles from its sensor data.
\todo[inline]{aggiungerei una breve descrizione di quello che accade nella simulazione in basso.}





\begin{figure}[h]
    \centering
    % First row
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/Simulations/sim2unkenv/frame_0.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/Simulations/sim2unkenv/frame_1.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/Simulations/sim2unkenv/frame_2.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/Simulations/sim2unkenv/frame_3.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/Simulations/sim2unkenv/frame_4.pdf}
    \end{subfigure}

    % Second row
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/Simulations/sim2unkenv/frame_5.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/Simulations/sim2unkenv/frame_6.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/Simulations/sim2unkenv/frame_7.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/Simulations/sim2unkenv/frame_8.pdf}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/Simulations/sim2unkenv/frame_9.pdf}
    \end{subfigure}

    \caption[short]{This is a description.\todo[inline]{aggiungerei descrizione degli elementi nelle figure (es. punti verdi, cerchio)}}
\end{figure}

\todo[inline]{IMPORTANTE: credo che bisogna aggiungere i plot sullo stato e l'input, e dire se hanno qualcosa di particolare o sono analoghi alle simulaizoni precedenti}